.\"                                      Hey, EMACS: -*- nroff -*-
.\" Man page for feature-selection
.\"
.\" Copyright (C) 2012 Linas Vepstas
.\"
.\" First parameter, NAME, should be all caps
.\" Second parameter, SECTION, should be 1-8, maybe w/ subsection
.\" other parameters are allowed: see man(7), man(1)
.pc
.TH FEATURE-SELECTION 1 "June 11, 2012" "3.1.8" "OpenCog Learning"
.LO 1
.\" Please adjust this date whenever revising the manpage.
.\"
.\" Some roff macros, for reference:
.\" .nh        disable hyphenation
.\" .hy        enable hyphenation
.\" .ad l      left justify
.\" .ad b      justify to both left and right margins
.\" .nf        disable filling
.\" .fi        enable filling
.\" .br        insert line break
.\" .sp <n>    insert n+1 empty lines
.\" for manpage-specific macros, see man(7)
.SH NAME
feature-selection \- dimensional reduction of datasets
.SH SYNOPSIS
.\" The help & version command line
.B feature-selection
.RB \-h | \--help
.br
.B feature-selection
.RB \--version
.br
.\" The general command line
.B feature-selection
.RB \-i
.IR filename
.RB [ \-a
.IR algo ]
.RB [ \-C
.IR num_features ]
.RB [ \-T
.IR threshold ]
.RB [ \-r
.IR random_seed ]
.RB [ \-u
.IR target_feature ]
.RB [ \-j
.IR jobs ]
.RB [ \-F
.IR log_file ]
.RB [ \-L ]
.RB [ \-l
.IR loglevel ]
.RB [ \-o
.IR output_file ]
.RB [ \-D
.IR fraction ]
.RB [ \-E
.IR tolerance ]
.RB [ \-U
.IR num_terms ]
.SH DESCRIPTION
.PP
.\" TeX users may be more comfortable with the \fB<whatever>\fP and
.\" \fI<whatever>\fP escape sequences to invoke bold face and italics,
.\" respectively.
\fBfeature-selection\fP is a command-line utility for performing
dimensional reduction of large datasets, so as to prepare them 
for a later regression analysis.  It is intended to be used to filter
data before it is fit with the \fBmoses\fP machine-learning program,
but it can be used in more general settings.  

.PP
.\" ============================================================
.SH EXAMPLES
.TP
.BI feature-selection\ \-i raw_data.csv \ \-ammi\ \-C 10 \ \-o reduced.csv \ \-F fs.log

Use the input file \fIraw_data.csv\fP as the input dataset.  The input
file must be a tab-separated or comma-separated table of values.  The 
dependent feature is assumed to be in the first column.  The \fBmmi\fP
algorithm will be used to select 10 different features that are the most
strongly correlated with the dependent feature.  The output will be
written to the file \fIreduced.csv\fP, consisting of the dependent
feature, in the first column, followed by the 10 most predictive
columns.  A logfile of algorithm progress will be written to
\fIfs.log\fP.

.TP
.BI feature-selection\ \-i raw_data.csv \ \-ammi\ \-C 10 \ \-o reduced.csv \ \-u Q3

As above, but use the column labelled \fIQ3\fP as the target feature.
This assumes that the input dataset contains columns labels as the first
row in the dataset.  The selected column labels are reproduced in the
output file.

.PP
.\" ============================================================
.SH OVERVIEW

In regression analysis, one has, in general, a table of values or
\'features\', and the goal of predicting one feature, the \'target\', 
(located in one column of the table) based on the values of the
other features (in other columns).  It is common that only some
of the features are useful predictors of the target, while the
others are mostly just noise.  Regression analysis becomes difficult
if there are too many input variables, and so regression can be improved
simply by filtering out the useless features. Doing this is generically
called \'feature selection\' or \'dimensional reduction\'.  This
program, \fBfeature-selection\fP, does this, using one of several
different algorithms.

In general, feature selection is done by looking for a correlation
between the target feature, and other features.  This correlation
can be computed in a number of different ways; this program uses
mutual information.  A strong correlation between the featureset
and the target variable suggests that the featureset is the 
appropriate one for performing regression analysis.

It can happen that two (or more) features are strongly correlated with
each-other, and that only one of these is needed as a predictor. Adding
any one of these features to the featureset will improve the
correlation considerably, but adding any of the others will have little
or no effect.  Such sets of features are called \'redundant\'.  A good
selection algorithm will select only one of the features of the
redundant set, omitting the others.

This program accepts inputs in a fairly generic whitespace or comma
delimited table format: whitespace may consist of repeated tabs or
blanks; values may be comma-separated but padded with blanks.
The input file may contain comment lines; these lines must start with 
a hash-mark (#), exclamation mark (!) or semi-colon (;) as the first
character.

Boolean values may be indicated with any of the characters 0,f,F and 
1,t,T.  Column values must be either boolean or numeric; at this time,
non-numeric data, including dates or columns containing currency symbols
are not supported.

.PP
.\" ============================================================
.SH OPTIONS
.PP
Options fall into three classes: options for controlling input and
output, options for controlling the algorithm behavior, and general
options.

.SS "General options"
.TP
.B \-h, \-\-help
Print command options and exit.
.TP
.BI \-j\  num \fR,\ \fB\-\-jobs= num
Allocate \fInum\fR threads for feature selection.  Using multiple
threads on multi-core processors will speed the feature selection
search.

.TP
.B -\-version
Print program version, and exit.

.PP
.\" ============================================================
.SS "Input and output control options"
These options control how the input and output is managed by the
program.

.TP
.BI \-F\  filename \fR,\ \fB\-\-log\-file= filename
Write debug log traces to \fIfilename\fR. If not specified, traces
are written to \fBfeature-selection.log\fR.  If the \-L option
is specified, this will be used as the base of the filename.
.TP
.BI \-i\  filename \fR,\ \fB\-\-input\-file= filename
The \fIfilename\fR specifies the input data file. The input table must
be in 'delimiter\-separated value' (DSV) format.  Valid separators
are comma (CSV, or comma-separated values), blanks and tabs
(whitespace). Columns correspond to features; there is one sample per
(non-blank) row. Comment characters are hash, bang and semicolon (#!;)
lines starting with a comment are ignored.
The \fB\-i\fR flag may be specified multiple times, to indicate multiple
input files. All files must have the same number of columns.
.TP
.BI \-L\fR,\ \fB\-\-log\-file\-dep\-opt
Write debug log traces to a filename constructed from the
\fIlogfile\fP specified in the \fB\-F\fP command, used as a prefix, 
and the other option flags and their values.  The filename will 
be truncated to a maximum of 255 characters.
.TP
.BI \-l\  loglevel \fR,\ \fB\-\-log\-level= loglevel
Specify the level of detail for debug logging. Possible
values for \fIloglevel\fR are \fBNONE\fR, \fBERROR\fR, \fBWARN\fR,
\fBINFO\fR, \fBDEBUG\fR, and \fBFINE\fR. Case does not matter.
Caution: excessive logging detail can lead to program slowdown.
.TP
.BI \-o\  filename \fR,\ \fB\-\-output\-file= filename
Write results to \fIfilename\fR. If not specified, results are written
to \fBstdout\fR.
.TP
.BI \-u\  label \fR,\ \fB\-\-target\-feature= label
The \fIlabel\fR is used as the target feature to fit.  If none is
specified, then the first column is used.  The very first row of the
input file, if it contains non-numeric, non-boolean values, is
interpreted as column labels, as is the common practice for
CSV/DSV file formats.
.PP
.\" ============================================================
.SS "Algorithm control options"
These options provide overall control over the algorithm execution.
The most important of these, for controlling behavior, are the
\fB\-a\fR, \fB\-C\fR and \fB\-T\fR flags.

.TP
.BI \-a\  algorithm \fR,\ \fB\-\-algo= algorithm
Select the algorithm to use for feature selection.
Available algorithms include:
.TS
tab (@);
l lx.
\fBmmi\fR@T{
Maximal Mutual Information.

This algorithm searches for the featureset with the highest mutual
information (MI) with regard to the target variable.  It does so by
adding one feature at a time to the featureset, computing the MI
between the target variable and this featureset,
ranking the result, and keeping only the highest-ranked results.
It can be thought of as a kind-of hill-climbing in the space
of mutual information.  This process is repeated until the desired
number of features is found, or until the MI score stops improving.

The maximum number of desired features must be specified with the
\fB\-C\fP option.  The \fB-T\fP option can be used to specify the
minimum desired improvement in the MI score.  That is, the algorithm
keeps adding features to the feature set until the improvement in the MI
score does not exceed this threshold.  Features are added in random
order, so that if there are redundant features, only one will be 
added, depending on the random seed given with the \fB\-r\fP option.

Two features are considered redundant if they are highly correlated,
so that adding either one of the two may improve MI a lot, but adding
the second will not.  Thus, only one is really needed; using the 
\fB\-T\fP option helps eliminate redundant features.

Note that when the number of desired features is large, this algo can
take a very long time to complete.  Although it can be much more accurate
than \fBinc\fP described below, it can be much much slower.
T}

\fBinc\fR@T{
Incremental, Non-Redundant Mutual Information.

Builds a featureset by incrementally adding features with the highest
mutual information with regard to the target.  Features are accepted
only if the mutual information is above a specified threshold. Features
are rejected if they appear to be redundant with others: that is,
if, by their presence, they fail to change the total mutual information
by more than a minimum amount.

One may specify either the number of features to be selected, or
one may specify a general "pressure" to automatically modulate the
number of features found.  That is, one must specify either the
\fB\-C\fP or the \fB\-T\fP flag, as otherwise, all features will
be selected.
T}

\fBhc\fR@T{
MOSES Hillclimbing.
T}
.TE
.TP
.BI \-C\  num_features \fR,\ \fB\-\-target\-size= num_features
Attempt to select at most \fInum_features\fR out of the input set.

This option is ignored unless the chosen algorithm is \fBmmi\fP or
\fBinc\fP.  When the selected algorithm is \fBinc\fP, then the 
\fB\-T\fP option is ignored.

.TP
.BI \-T\  threshold \fR,\ \fB\-\-threshold= threshold
Apply a floating-point threshold for selecting a feature.
A positive value prevents low-scoring features from being 
selected; a value of zero or less will accept all features.

This option is ignored unless the chosen algorithm is 
\fBmmi\fP or \fBinc\fP.

.TP
.BI \-r\  seed \fR,\ \fB\-\-random\-seed= seed
Use \fIseed\fR as the seed value for the pseudo-random number generator.
The various algorithms use the random number generator in different
ways.  The \fPmmi\fP algorithm explicitly shuffles features, so that 
if the dataset contains multiple redundant features, one will be 
chosen randomly.

.\" ============================================================
.SS "Incremental algorithm options"
These options only apply to the \fB\-ainc\fP algorithm.

.TP
.BI \-D\  fraction \fR,\ \fB\-\-int\-redundant\-intensity= fraction
Threshold fraction used to reject redundant features. If a feature
contributes less than \fIfraction\fP * \fIthreshold\fP to the total
score, it will be rejected from the final feature set.  That is, if
two features are strongly correlated, one should be considered 
redundant; as to which is de-selected will depend on the random-number
generator, i.e. on the random seed specified with the \fB\-r\fP option.

.TP
.BI \-E\  tolerance \fR,\ \fB\-\-inc\-target\-size\-epsilon= tolerance
To be used only with the \fB\-C\fP option.  The incremental algorithm
is not able to directly select a fixed number of features; rather, it
dynamically adjusts the threshold until the desired number of features
results. This option controls the smallest adjustment made.

.TP
.BI \-U\  num_terms \fR,\ \fB\-\-inc\-interaction\-terms= num_terms
The number of variables used in computing the joint entropy.  Normally,
this algorithm never computes the joint entropy of multiple features;
it only considers the effect of a single feature at a time on the
target (that is, it only computes the mutual information between one
feature and the target).  Specifying a number greater than one will
consider the mutual information between multiple features and the
target.  Note that using this calculation is combinatorially more 
computationally expensive, as all possible choices are considered.
That is, (n choose k) choices are considered, where n==number of
features, and k==number of interaction terms.
.PP
.\" ============================================================
.SH TODO
Document the MOSES-algorithm and the options that it takes: -A -c -f -m
-O -s.  These are not documented because the hill-climbing algo is
currently not supported.

.SH SEE ALSO
.br
More information is available at
.B http://wiki.opencog.org/w/Feature_selection
.SH AUTHORS
.nh
\fBfeature-selection\fP was written by Nil Geisweiller and modified by
Linas Vepstas
.PP
This manual page is being written by Linas Vepstas.

<!DOCTYPE html>
<!-- saved from url=(0062)http://blog.opencog.org/2012/03/20/genetic-crossover-in-moses/ -->
<html dir="ltr" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Genetic Crossover in MOSES | OpenCog Brainwave</title>
<link rel="stylesheet" type="text/css" media="all" href="./Genetic Crossover_files/style.css">
				
<link rel="canonical" href="./Genetic Crossover_files/Genetic Crossover.html">
</head>

<body class="single single-post postid-370">
<div id="wrapper" class="hfeed">

	<div id="main">

		<div id="container">
			<div id="content" role="main">


				<div id="post-370" class="post-370 post type-post hentry category-design category-documentation category-introduction category-theory tag-genetic-algorithm tag-hillclimbing tag-learning tag-machine-learning tag-meta-learning tag-moses">
					<h1 class="entry-title">Genetic Crossover in MOSES</h1>

					<div class="entry-meta">
						<span class="meta-prep meta-prep-author">Posted on</span> <a href="./Genetic Crossover_files/Genetic Crossover.html" title="6:59 pm" rel="bookmark"><span class="entry-date">March 20, 2012</span></a> <span class="meta-sep">by</span> <span class="author vcard"><a class="url fn n" href="http://blog.opencog.org/author/linasv/" title="View all posts by Linas Vepstas">Linas Vepstas</a></span>					</div><!-- .entry-meta -->

					<div class="entry-content">
						<p><a href="http://wiki.opencog.org/w/MOSES">MOSES</a> is a system for learning programs from input data. &nbsp;Given a table of input values, and a column of outputs, MOSES tries to learn a program, the simplest program that can reproduce the output given the input values.  The programs that it learns are in the form of a “program tree” — &nbsp;a nested concatenation of operators, such as addition or multiplication, boolean AND’s or OR’s, if-statements, and the like, taking the inputs as arguments. &nbsp;To learn a program, it starts by guessing a new random program. &nbsp;More precisely, it generates a new, random program tree, with as-yet unspecified operators at the nodes of the tree. So, for example, an arithmetic node maybe be addition, or subtraction, or multiplication, division, or it may be entirely absent. &nbsp;It hasn’t yet been decided which. &nbsp; In MOSES, each such undecided node is termed a “knob”, and program learning is done by “turning the knobs” until a reasonable program is found. &nbsp;But things don’t stop there: once a “reasonable” program is found, a new, random program tree is created by decorating this “most reasonable” program with a new set of knobs. &nbsp;The process then repeats: knobs are turned until an even better program is found.</p>
<p>Thus, MOSES is a “metalearning” system: it consists of an outer loop, that creates trees and knobs, and an inner loop, that finds optimal knob settings. &nbsp;Both loops “learn” or “optimize”; it is the nesting of these that garners the name “metalearning”.  Each loop can use completely different optimization algorithms in its search for optimal results.</p>
<p>The rest of this post concerns this inner loop, and making sure that it finds optimal knob settings as quickly and efficiently as possible.  The space of all possible knob settings is large: if, for example, each knob has 5 possible settings, and there are 100 knobs, then there is a total of 5<sup>100</sup> possible different settings: a combinatorial explosion.&nbsp;Such spaces are hard to search. There are a variety of different algorithms for exploring such a space.  One very simple, very traditional algorithm is “hillclimbing”.  This algo starts somewhere in this space, at a single point, say, the one with all the knobs set to zero.  It then searches the entire local neighborhood of this point: each knob is varied, one at a time, and a score is computed.  Of these scores, one will be best. The corresponding knob setting is then picked a the new center, and the process then repeats; it repeats until there is no improvement: until one can’t “climb up this hill” any further.  At this point, the inner loop is done; the “best possible” program has been found, and control is returned to the outer loop.</p>
<p>Hill-climbing is a rather stupid algorithm: most knob settings will result in terrible scores, and are pointless to explore, but the hill-climber does so anyway, as it has no clue as to where the “good knobs” lie.  It does an exhaustive search of the local neighborhood of single-knob twists.&nbsp; One can do much better by using estimation-of-distribution algorithms, such as the Bayesian Optimization Algorithm. &nbsp;The basic premise is that knob settings are correlated: good settings are near other good settings. &nbsp;By collecting statistics and computing probabilities, one can make informed, competent guesses at which knob settings might actually be good. &nbsp;The downside to such algorithms is that they are complex: &nbsp;the code is hard to write, hard to debug, and slow to run: there is a performance penalty for computing those “educated guesses”.</p>
<p>This post explores a middle ground: a genetic cross-over algorithm that improves on simple hill-climbing simply by blindly assuming that good knob settings really are “near each other”, without bothering to compute any probabilities to support this rash assumption. &nbsp;The algorithm works; headway can be made by exploring only the small set of knob settings that correlate with previous good knob settings.</p>
<p>To explain this, it is time to take a look at some typical “real-life” data. In what follows, a dataset was collected from a customer-satisfaction survey; the goal is to predict satisfaction from a set of customer responses. &nbsp;The dataset is a table; the outer loop has generated a program decorated with a set of knobs. &nbsp;Starting with some initial knob setting, we vary each knob in turn, and compute the score. The first graph below shows what a &nbsp;typical “nearest neighborhood” looks like. &nbsp;The term “nearest neighborhood” simply means that, starting with the initial knob setting, the nearest neighbors are those that differ from it by exactly one knob setting, and no more. &nbsp;There is also a <em>distance</em>=2 neighborhood: those instances that differ by exactly two knob settings from the “center” instance. &nbsp;Likewise, there is a <em>distance</em>=3 neighborhood, differing by 3 knob settings,<em> etc. </em> The size of each neighborhood gets combinatorially larger. &nbsp;So, if there are 100 knobs, and each knob has five settings, then there are 5 × 100=500 nearest neighbors.  There are 500 × 499 / 2 = 125K next-nearest neighbors, and 500 × 499 × 498 / (2 × 3) = 21M instances at <em>distance</em>=3.  In general, this is the binomial coefficient: (500 choose <em>k</em>) for distance <em>k</em>.  Different knobs, however, may have more or fewer than just 5 settings, so the above is just a rough example.</p>
<div id="attachment_380" class="wp-caption alignnone" style="width: 650px"><a href="./Genetic Crossover_files/hc-20.png"><img class="size-full wp-image-380" src="./Genetic Crossover_files/hc-20.png" alt="Nearest Neighbor Scores" width="640" height="480"></a><p class="wp-caption-text">Nearest Neighbor Scores</p></div>
<p>The above graph shows the distribution of nearest neighbor scores, for a “typical” neighborhood. The score of the center instance (the center of the neighborhood) is indicated by the solid green line running across the graph, labelled “previous high score”. &nbsp;All of the other instances differ by exactly one knob setting from this center. &nbsp;They’ve been scored and ranked, so that the highest-scoring neighbors are to the left. &nbsp;As can be seen, there are maybe 15 instances with higher scores than the center, another 5 that seem to tie. &nbsp;A slow decline is followed by a precipitous drop; there are another 80 instances with scores so bad that they are not shown in this figure. &nbsp;The hill-climbing algo merely picks the highest scorer, declares it to be the new center, and repeats the process.</p>
<p>All of the other neighborhoods look substantially similar. The graph below shows an average over many generations (here, each iteration of the inner loop is one generation).  The jaggedness above is smoothed out by averaging.</p>
<div id="attachment_393" class="wp-caption alignnone" style="width: 650px"><a href="./Genetic Crossover_files/distrib-avg-bank-r01.png"><img class="size-full wp-image-393" src="./Genetic Crossover_files/distrib-avg-bank-r01.png" alt="Nearest Neighbor Score Change" width="640" height="480"></a><p class="wp-caption-text">Nearest Neighbor Score Change</p></div>
<p>Rather than searching the entire neighborhood, one would like to test only those knob settings likely to yield good scores. But which might these be? &nbsp;For nearest neighbors, there is no way to tell, without going through the bother of collecting statistics, and running them through some or another Bayesian estimation algorithm.</p>
<p>However, for more distant neighbors, there is a way of guessing and getting lucky: perform genetic cross-overs. &nbsp;That is, take the highest and next-highest scoring instances, and create a new instance that differs from the center by two knob-settings, the two knobs associated with the two high scorers. &nbsp;In fact, this new instance will very often be quite good, beating both of its parents. &nbsp; The graph below shows what happens when we cross the highest scorer with each one of the next 70 highest. The label “1-simplex” simply reminds us that these instances differ by exactly two knob settings from the center. &nbsp;More on simplexes later. &nbsp;The green zero line is located at the highest-scoring single-knob change. &nbsp;The graph shows that by starting here, and twiddling the next-most-promising knob, can often be a win.  Not always: in the graph below, only 4 different knobs showed improvement.  However, we explored relatively few instances to find these four; for this dataset, most exemplars have thousands of knobs.</p>
<div id="attachment_391" class="wp-caption alignnone" style="width: 650px"><a href="./Genetic Crossover_files/distrib-avg-bank-1-plex1.png"><img class="size-full wp-image-391" src="./Genetic Crossover_files/distrib-avg-bank-1-plex1.png" alt="Average Score Change, 1-simplex" width="640" height="480"></a><p class="wp-caption-text">Average Score Change, 1-simplex</p></div>
<p>The take-away lesson here is that we can avoid exhaustive searches by simply crossing the 10 or 20 or 30 best instances, and hoping for the best. In fact, we get lucky with these guesses quite often.  What happens if, instead of just crossing two, we cross three of the top scorers? &nbsp;This is the “2-simplex”, below:</p>
<div id="attachment_390" class="wp-caption alignnone" style="width: 650px"><a href="./Genetic Crossover_files/distrib-avg-bank-2-plex1.png"><img class="size-full wp-image-390" src="./Genetic Crossover_files/distrib-avg-bank-2-plex1.png" alt="Average Score Change" width="640" height="480"></a><p class="wp-caption-text">Average Score Change, 2-simplex</p></div>
<p>Notice that there are now even more excellent candidates! &nbsp;How far can we go? &nbsp;The 3-simplex graph below shows the average score change from crossing over four high-scoring instances:</p>
<div id="attachment_388" class="wp-caption alignnone" style="width: 650px"><a href="./Genetic Crossover_files/distrib-avg-bank-3-plex.png"><img class="size-full wp-image-388" src="./Genetic Crossover_files/distrib-avg-bank-3-plex.png" alt="Average Score Change" width="640" height="480"></a><p class="wp-caption-text">Average Score Change. 3-simplex</p></div>
<p>The term “crossover” suggests some sort of “sexual genetic reproduction”. While this is correct, it is somewhat misleading. &nbsp; The starting population is genetically very uniform, with little “genetic variation”. &nbsp;The algorithm starts with one single “grandparent”, and produces a population of “parents”, each of which differ from the grandparent by exactly one knob setting.  In the “nearest neighborhood” terminology, the “grandparent” is the “center”, and each “parent” is exactly one step away from this center.  Any two “parents”, arbitrarily chosen, will always differ from one-another by exactly two knob settings.  Thus, crossing over two parents will produce a child that differs by exactly one knob setting from each parent, and by two from the grandparent.   In the “neighborhood” model, this child is a distance=2 from the grandparent. &nbsp; For the case of &nbsp;three parents, the child is at distance=3 from the grandparent, and so on: four parents produce a child that is distance=4 from the grandparent. &nbsp;Thus, while “sexual reproduction” is a sexy term, it looses its punch with the rather stark uniformity of the parent population; thinking in terms of “neighbors” and “distance” provides a more accurate mental model of what is happening here.</p>
<p>The term “simplex” used above refers to the shape of the iteration over the ranked instances: a 1-simplex is a straight line segment, a 2-simplex is a right triangle, a 3-simplex is a right tetrahedron.  The iteration is performed with 1, 2 or 3 nested loops that cross over 1, 2 or 3 instances against the highest.  It is important to notice that the loops do not run over the entire range of nearest neighbors, but only over the top scoring ones.   So, for example, crossing over the 7 highest-scoring instances for the 3-simplex generates 6!/(6-3)! = 6 × 5 × 4 = 120 candidates.  Scoring a mere 120 candidates can be very quick, as compared to an exhaustive search of many thousands of nearest neighbors.  Add to this the fact that most of the 120 are likely to score quite well, whereas only a tiny handful of the thousands of nearest neighbors will show any improvement, and the advantage of this guessing game is quite clear.</p>
<p>So what is it like, after we put it all together? The graph below shows the score as a function of runtime.</p>
<div id="attachment_397" class="wp-caption alignnone" style="width: 650px"><a href="./Genetic Crossover_files/deme-tri-ti.png"><img class="size-full wp-image-397" src="./Genetic Crossover_files/deme-tri-ti.png" alt="Score as function of time" width="640" height="480"></a><p class="wp-caption-text">Score as function of time</p></div>
<p>In the above graph, each tick mark represents one generation. The long horizontal stretches between tick marks shows the time taken to perform an exhaustive nearest-neighborhood search.  For the first 100 seconds or so, the exemplar has very few knobs in it (a few hundred), and so an exhaustive search is quick and easy.  After this point, the exemplars get dramatically more complex, and consist of thousands of knobs.   At this point, an exhaustive neighborhood search becomes expensive: about 100 seconds or so, judging from the graph.  While the exhaustive search is always finding an improvement for this dataset, it is clear that performing some optimistic guessing can improve the score a good bit faster.  As can be seen from this graph, the algorithm falls back to an exhaustive search when the optimistic simplex-based guessing fails to show improvement; it then resumes with guessing.</p>
<p>To conclude: for many kinds of datasets, a very simple genetic-crossover algorithm combined with hillclimbing can prove a simple but effective search algorithm.</p>
<p><em>Note Bene</em>: the above only works for some problem types; thus it is not (currently) enabled by default. To turn it on, specify the -Z1 flag when invoking moses.</p>
<h2>Appendix</h2>
<p>Just to keep things honest, and to show some of the difficulty of algorithm tuning, below is a graph of some intermediate results taken during the work. &nbsp;I won’t explain what they all are, but do note one curious feature: &nbsp;the algos which advance the fastest initially seem to have trouble advancing later on. &nbsp;This suggests a somewhat “deceptive” scoring landscape: the strong early advancers get trapped in local maxima that they can’t escape. &nbsp; The weak early advancers somehow avoid these traps. &nbsp;Note also that results have some fair dependence on the random number generator seed; different algos effectively work with different random sequences, and so confuse direct comparison by some fair bit.</p>
<div id="attachment_415" class="wp-caption alignleft" style="width: 1034px"><a href="./Genetic Crossover_files/deme-tri-more.png"><img class="size-full wp-image-415" src="./Genetic Crossover_files/deme-tri-more.png" alt="Many Different Algorithms" width="1024" height="768"></a><p class="wp-caption-text">Many Different Algorithms</p></div>
</div><!-- .entry-content -->

				</div><!-- #post-## -->

		</div><!-- #primary .widget-area -->

	</div><!-- #main -->

</div><!-- #wrapper -->

</div>

</body></html>
